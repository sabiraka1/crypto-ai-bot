import pandas as pd
import numpy as np
import joblib
import os
import logging
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

MODEL_PATH = "models/ai_model.pkl"
OLD_MODEL_PATH = "models/ai_model_backup.pkl"
CSV_FILE = "sinyal_fiyat_analizi.csv"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    if not os.path.exists(CSV_FILE):
        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {CSV_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None, None, None
    
    try:
        df = pd.read_csv(CSV_FILE)
        logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_cols = ["signal", "rsi", "macd", "success"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
            return None, None, None
        
        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = df.dropna(subset=required_cols)
        
        if len(df) < 10:
            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {len(df)} –∑–∞–ø–∏—Å–µ–π")
            return None, None, None
        
        # –£–ë–†–ê–ù–ê –ø—Ä–æ–±–ª–µ–º–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å ema_signal!
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df["signal_encoded"] = df["signal"].map({
            "BUY": 1, "STRONG_BUY": 1.5, 
            "SELL": -1, "STRONG_SELL": -1.5, 
            "CRITICAL_SELL": -2,
            "HOLD": 0, "WAIT": 0, "NONE": 0, "ERROR": 0
        }).fillna(0)
        
        df["result_encoded"] = df["success"].astype(int)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        feature_cols = ["rsi", "macd", "signal_encoded"]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        optional_features = [
            "pattern_score", "confidence", "buy_score", "sell_score",
            "total_score", "macd_contribution", "ai_score",
            "price_change_24h", "macd_histogram"
        ]
        
        for feat in optional_features:
            if feat in df.columns:
                df[feat] = pd.to_numeric(df[feat], errors='coerce').fillna(0)
                feature_cols.append(feat)
                logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫: {feat}")
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
        if "pattern_direction" in df.columns:
            df["pattern_direction_encoded"] = df["pattern_direction"].map({
                "BULLISH": 1, "BEARISH": -1, "REVERSAL": 0.5, 
                "INDECISION": 0, "NEUTRAL": 0
            }).fillna(0)
            feature_cols.append("pattern_direction_encoded")
            logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ pattern_direction")
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è Enhanced —Å–∏—Å—Ç–µ–º—ã
        if "trend_1d" in df.columns:
            df["trend_1d_encoded"] = df["trend_1d"].map({
                "BULLISH": 1, "BEARISH": -1, "NEUTRAL": 0, "UNKNOWN": 0
            }).fillna(0)
            feature_cols.append("trend_1d_encoded")
            logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ trend_1d")
            
        if "trend_4h" in df.columns:
            df["trend_4h_encoded"] = df["trend_4h"].map({
                "BULLISH": 1, "BEARISH": -1, "NEUTRAL": 0, "UNKNOWN": 0
            }).fillna(0)
            feature_cols.append("trend_4h_encoded")
            logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ trend_4h")
        
        if "market_state" in df.columns:
            df["market_state_encoded"] = df["market_state"].map({
                "NORMAL": 0, "HIGH_VOLATILITY": 0.5, 
                "OVERHEATED_BULLISH": 1, "OVERSOLD_BEARISH": -1,
                "OVERHEATED": 1
            }).fillna(0)
            feature_cols.append("market_state_encoded")
            logger.info("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ market_state")
        
        X = df[feature_cols]
        y = df["result_encoded"]
        
        logger.info(f"üìà –ü—Ä–∏–∑–Ω–∞–∫–∏ ({len(feature_cols)}): {feature_cols}")
        logger.info(f"üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {y.value_counts().to_dict()}")
        
        return X, y, feature_cols
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        logger.error(f"‚ùå –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns) if 'df' in locals() else 'N/A'}")
        return None, None, None

def train_model():
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    logger.info("üß† –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    X, y, feature_names = load_data()
    
    if X is None or len(X) < 20:
        logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏!")
        
        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ
        if not os.path.exists(MODEL_PATH):
            create_basic_model()
        return
    
    try:
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫
        stratify_param = y if len(y.unique()) > 1 else None
        test_size = min(0.2, max(0.1, 10 / len(X)))  # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–∞
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=stratify_param
        )
        
        # –û–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        new_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞
        )
        
        new_model.fit(X_train, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        y_pred = new_model.predict(X_test)
        new_acc = accuracy_score(y_test, y_pred)
        logger.info(f"üìà –¢–æ—á–Ω–æ—Å—Ç—å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {new_acc:.3f}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏
        old_acc = 0
        if os.path.exists(MODEL_PATH):
            try:
                old_model = joblib.load(MODEL_PATH)
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                if hasattr(old_model, 'n_features_in_') and old_model.n_features_in_ == X_test.shape[1]:
                    y_pred_old = old_model.predict(X_test)
                    old_acc = accuracy_score(y_test, y_pred_old)
                    logger.info(f"üìâ –¢–æ—á–Ω–æ—Å—Ç—å —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏: {old_acc:.3f}")
                else:
                    logger.warning("‚ö†Ô∏è –°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞ - –±—É–¥–µ—Ç –∑–∞–º–µ–Ω–µ–Ω–∞")
                    old_acc = 0
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –æ–Ω–∞ –ª—É—á—à–µ
        if new_acc >= old_acc or not os.path.exists(MODEL_PATH):
            # –†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏
            if os.path.exists(MODEL_PATH):
                try:
                    os.rename(MODEL_PATH, OLD_MODEL_PATH)
                    logger.info("üóÇÔ∏è –°—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            os.makedirs("models", exist_ok=True)
            joblib.dump(new_model, MODEL_PATH)
            logger.info("‚úÖ –ù–æ–≤–∞—è AI-–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            try:
                logger.info("\n" + classification_report(y_test, y_pred))
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç—á–µ—Ç–∞: {e}")
            
        else:
            logger.warning("‚ùå –ù–æ–≤–∞—è –º–æ–¥–µ–ª—å —Ö—É–∂–µ —Å—Ç–∞—Ä–æ–π - –æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä—É—é")
        
        # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        try:
            importances = new_model.feature_importances_
            logger.info("üìä –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            for name, score in zip(feature_names, importances):
                logger.info(f"  {name}: {score:.3f}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏: {e}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        try:
            plt.figure(figsize=(12, 6))
            plt.bar(feature_names, importances, color='green', alpha=0.7)
            plt.title("üîç –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–æ–¥–µ–ª–∏")
            plt.xlabel("–ü—Ä–∏–∑–Ω–∞–∫–∏")
            plt.ylabel("–í–∞–∂–Ω–æ—Å—Ç—å")
            plt.xticks(rotation=45)
            plt.tight_layout()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
            os.makedirs("charts", exist_ok=True)
            plt.savefig("charts/feature_importance.png", dpi=300, bbox_inches='tight')
            plt.close()
            logger.info("üìä –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")

def create_basic_model():
    """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å –ø—Ä–∏–º–µ—Ä–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    logger.info("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
    
    try:
        # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        data = {
            "rsi": [25, 70, 45, 80, 30, 65, 50, 40, 60, 35, 75, 28, 55, 85, 20],
            "macd": [0.5, -0.3, 0.1, -0.4, 0.6, -0.2, 0.0, 0.3, -0.1, 0.4, -0.5, 0.7, 0.2, -0.6, 0.8],
            "signal": ["BUY", "SELL", "HOLD", "SELL", "BUY", "SELL", "HOLD", "BUY", "SELL", "BUY", "SELL", "BUY", "HOLD", "SELL", "BUY"],
            "success": [1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1]
        }
        
        df = pd.DataFrame(data)
        df["signal_encoded"] = df["signal"].map({"BUY": 1, "SELL": -1, "HOLD": 0})
        df["pattern_score"] = np.random.uniform(0, 6, len(df))
        df["confidence"] = np.random.uniform(20, 90, len(df))
        df["total_score"] = np.random.uniform(0, 5, len(df))
        df["macd_contribution"] = np.random.uniform(0, 3, len(df))
        df["ai_score"] = np.random.uniform(0.1, 0.9, len(df))
        
        feature_cols = ["rsi", "macd", "signal_encoded", "pattern_score", "confidence", 
                       "total_score", "macd_contribution", "ai_score"]
        X = df[feature_cols]
        y = df["success"]
        
        # –û–±—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X, y)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        os.makedirs("models", exist_ok=True)
        joblib.dump(model, MODEL_PATH)
        logger.info("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e}")

def retrain_model():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç–∞)"""
    logger.info("üîÅ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ AI-–º–æ–¥–µ–ª–∏...")
    try:
        train_model()
        logger.info("‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {e}")
        # –ù–ï –ø–æ–¥–Ω–∏–º–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –Ω–µ –∫—Ä–∞—à–∏—Ç—å –±–æ—Ç–∞
        logger.info("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É —Å–æ —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª—å—é")

def get_model_info():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
    if not os.path.exists(MODEL_PATH):
        return "‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    
    try:
        model = joblib.load(MODEL_PATH)
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        info = {
            "type": type(model).__name__,
            "n_estimators": getattr(model, 'n_estimators', 'N/A'),
            "max_depth": getattr(model, 'max_depth', 'N/A'),
            "n_features": getattr(model, 'n_features_in_', 'N/A')
        }
        
        return info
        
    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}"

if __name__ == "__main__":
    train_model()
