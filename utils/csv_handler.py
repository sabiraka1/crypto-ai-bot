# utils/csv_handler.py - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –° –ë–ê–¢–ß–ò–ù–ì–û–ú

import csv
import os
import logging
import pandas as pd
import threading
import time
from datetime import datetime
from typing import List, Dict, Any, NamedTuple
from collections import deque
from config.settings import CLOSED_TRADES_CSV, SIGNALS_CSV, LOGS_DIR

# =============================================================================
# –°–ò–°–¢–ï–ú–ê –ë–ê–¢–ß–ò–ù–ì–ê CSV –ó–ê–ü–ò–°–ï–ô
# =============================================================================

class CSVRecord(NamedTuple):
    """–ó–∞–ø–∏—Å—å –¥–ª—è –±–∞—Ç—á–∏–Ω–≥–∞"""
    file_path: str
    fieldnames: List[str] 
    data: Dict[str, Any]

class BatchCSVWriter:
    """–ë–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –≤ CSV —Ñ–∞–π–ª—ã"""
    
    def __init__(self, batch_size: int = 10, flush_interval: float = 30.0):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self._buffer = deque()
        self._lock = threading.RLock()
        self._last_flush = time.time()
        self._total_records = 0
        self._flush_count = 0
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–π —Ñ–ª—É—à–µ—Ä
        self._start_background_flusher()
        
    def add_record(self, file_path: str, fieldnames: List[str], data: Dict[str, Any]):
        """–î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å—å –≤ –±—É—Ñ–µ—Ä"""
        with self._lock:
            self._buffer.append(CSVRecord(file_path, fieldnames, data))
            self._total_records += 1
            
            # –ù–µ–º–µ–¥–ª–µ–Ω–Ω—ã–π flush –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞
            if len(self._buffer) >= self.batch_size:
                self._flush_buffer()
    
    def _start_background_flusher(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ñ–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å–±—Ä–æ—Å–∞ –±—É—Ñ–µ—Ä–∞"""
        def background_flush():
            while True:
                time.sleep(self.flush_interval)
                with self._lock:
                    if self._buffer and time.time() - self._last_flush > self.flush_interval:
                        self._flush_buffer()
        
        thread = threading.Thread(target=background_flush, daemon=True, name="CSVBatchFlusher")
        thread.start()
        logging.debug("üìÑ Background CSV flusher started")
    
    def _flush_buffer(self):
        """–°–±—Ä–æ—Å–∏—Ç—å –±—É—Ñ–µ—Ä –≤ —Ñ–∞–π–ª—ã"""
        if not self._buffer:
            return
            
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ –ø–æ —Ñ–∞–π–ª–∞–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        by_file = {}
        buffer_size = len(self._buffer)
        
        while self._buffer:
            record = self._buffer.popleft()
            if record.file_path not in by_file:
                by_file[record.file_path] = {
                    'fieldnames': record.fieldnames,
                    'records': []
                }
            by_file[record.file_path]['records'].append(record.data)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –ø–æ —Ñ–∞–π–ª–∞–º
        for file_path, file_data in by_file.items():
            try:
                self._write_batch_to_file(file_path, file_data['fieldnames'], file_data['records'])
            except Exception as e:
                logging.error(f"Batch CSV write failed for {file_path}: {e}")
        
        self._last_flush = time.time()
        self._flush_count += 1
        
        logging.debug(f"üìÑ CSV batch flushed: {buffer_size} records to {len(by_file)} files")
    
    def _write_batch_to_file(self, file_path: str, fieldnames: List[str], records: List[Dict[str, Any]]):
        """–ó–∞–ø–∏—Å–∞—Ç—å –±–∞—Ç—á –∑–∞–ø–∏—Å–µ–π –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª"""
        if not records:
            return
            
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ —Ñ–∞–π–ª –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        os.makedirs(os.path.dirname(file_path) or ".", exist_ok=True)
        
        file_exists = os.path.isfile(file_path)
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–æ–≤—ã–π
        if not file_exists:
            with open(file_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ –æ–¥–Ω–∏–º –±–ª–æ–∫–æ–º
        with open(file_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            for record_data in records:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–æ–ª—è–º –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç—ã–µ
                filtered_data = {}
                for field in fieldnames:
                    value = record_data.get(field, "")
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è CSV
                    if isinstance(value, (int, float)) and not isinstance(value, bool):
                        filtered_data[field] = str(value) if pd.notna(value) else ""
                    else:
                        filtered_data[field] = str(value) if value is not None else ""
                        
                writer.writerow(filtered_data)
    
    def force_flush(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–æ—Å–∏—Ç—å –±—É—Ñ–µ—Ä"""
        with self._lock:
            self._flush_buffer()
    
    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á–µ—Ä–∞"""
        with self._lock:
            return {
                "buffer_size": len(self._buffer),
                "total_records": self._total_records,
                "flush_count": self._flush_count,
                "last_flush": self._last_flush,
                "batch_size": self.batch_size,
                "flush_interval": self.flush_interval
            }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –±–∞—Ç—á–µ—Ä
_csv_batcher = BatchCSVWriter(batch_size=15, flush_interval=20.0)

# =============================================================================
# –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô CSV HANDLER
# =============================================================================

class CSVHandler:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ CSV —Å –±–∞—Ç—á–∏–Ω–≥–æ–º –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Å–∏–≥–Ω–∞–ª–æ–≤
    SIGNALS_FIELDS = [
        "timestamp", "symbol", "timeframe", "close", 
        "buy_score", "ai_score", "market_condition", "decision", "reason"
    ]

    # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Å–¥–µ–ª–æ–∫  
    TRADES_FIELDS = [
        "timestamp_open", "timestamp_close", "symbol", "side",
        "entry_price", "exit_price", "qty_usd", "pnl_pct", "pnl_abs", 
        "duration_minutes", "reason", "buy_score", "ai_score"
    ]

    # –ö—ç—à –¥–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
    _read_cache = {}
    _cache_ttl = 30  # —Å–µ–∫—É–Ω–¥
    
    @staticmethod
    def log_signal_snapshot(data: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞ —Å –±–∞—Ç—á–∏–Ω–≥–æ–º"""
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º timestamp –µ—Å–ª–∏ –Ω–µ—Ç
            if "timestamp" not in data:
                data["timestamp"] = datetime.now().isoformat()
                
            _csv_batcher.add_record(SIGNALS_CSV, CSVHandler.SIGNALS_FIELDS, data)
            logging.debug("üìä Signal logged to batch")
            
        except Exception as e:
            logging.error(f"Failed to log signal: {e}")
    
    @staticmethod  
    def log_close_trade(data: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–∫—Ä—ã—Ç–æ–π —Å–¥–µ–ª–∫–∏ —Å –±–∞—Ç—á–∏–Ω–≥–æ–º"""
        try:
            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ —É–ø—Ä–æ—â–µ–Ω–Ω—É—é —Å—Ö–µ–º—É
            adapted = {
                "timestamp_open": data.get("entry_ts", ""),
                "timestamp_close": data.get("close_ts", datetime.now().isoformat()),
                "symbol": data.get("symbol", ""),
                "side": data.get("side", "LONG"),
                "entry_price": data.get("entry_price", 0.0),
                "exit_price": data.get("exit_price", 0.0),
                "qty_usd": data.get("qty_usd", 0.0),
                "pnl_pct": data.get("pnl_pct", 0.0),
                "pnl_abs": data.get("pnl_abs", 0.0),
                "duration_minutes": data.get("duration_minutes", 0.0),
                "reason": data.get("reason", ""),
                "buy_score": data.get("buy_score"),
                "ai_score": data.get("ai_score")
            }
            
            _csv_batcher.add_record(CLOSED_TRADES_CSV, CSVHandler.TRADES_FIELDS, adapted)
            logging.debug("üí∞ Trade logged to batch")
            
        except Exception as e:
            logging.error(f"Failed to log trade: {e}")

    @staticmethod
    def read_csv_cached(file_path: str, use_cache: bool = True) -> List[Dict[str, Any]]:
        """–ß—Ç–µ–Ω–∏–µ CSV —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not os.path.exists(file_path):
            return []
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache and file_path in CSVHandler._read_cache:
            cached_data, cached_time, cached_mtime = CSVHandler._read_cache[file_path]
            current_mtime = os.path.getmtime(file_path)
            
            # –ö—ç—à –≤–∞–ª–∏–¥–µ–Ω –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è –∏ –Ω–µ –∏—Å—Ç–µ–∫ TTL
            if (time.time() - cached_time < CSVHandler._cache_ttl and 
                current_mtime == cached_mtime):
                return cached_data.copy()
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        try:
            with open(file_path, mode="r", newline="", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                data = list(reader)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            if use_cache:
                mtime = os.path.getmtime(file_path)
                CSVHandler._read_cache[file_path] = (data.copy(), time.time(), mtime)
            
            return data
            
        except Exception as e:
            logging.error(f"Failed to read CSV {file_path}: {e}")
            return []

    @staticmethod
    def read_csv_safe(file_path: str) -> List[Dict[str, Any]]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        return CSVHandler.read_csv_cached(file_path, use_cache=True)

    @staticmethod
    def read_last_trades(limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–¥–µ–ª–∫–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        try:
            trades = CSVHandler.read_csv_cached(CLOSED_TRADES_CSV, use_cache=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ —Å–¥–µ–ª–∫–∏
            completed = []
            for trade in trades:
                exit_price = trade.get("exit_price", "")
                if exit_price and str(exit_price).strip() and str(exit_price) != "0.0":
                    completed.append(trade)
            
            return completed[-limit:] if completed else []
            
        except Exception as e:
            logging.error(f"Failed to get last trades: {e}")
            return []

    @staticmethod
    def get_trade_stats() -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–¥–µ–ª–∫–∞–º"""
        try:
            trades = CSVHandler.read_csv_cached(CLOSED_TRADES_CSV, use_cache=True)
            if not trades:
                return {"count": 0, "win_rate": 0, "avg_pnl": 0}

            # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Ä–∞—Å—á–µ—Ç
            valid_pnl = []
            for trade in trades:
                exit_price = trade.get("exit_price", "")
                if exit_price and str(exit_price).strip():
                    try:
                        pnl = float(trade.get("pnl_pct", 0))
                        valid_pnl.append(pnl)
                    except (ValueError, TypeError):
                        continue

            if not valid_pnl:
                return {"count": 0, "win_rate": 0, "avg_pnl": 0}

            # –ë—ã—Å—Ç—Ä—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
            import numpy as np
            pnl_array = np.array(valid_pnl)
            wins = pnl_array[pnl_array > 0]
            
            return {
                "count": len(valid_pnl),
                "win_rate": round((len(wins) / len(valid_pnl)) * 100, 2),
                "avg_pnl": round(np.mean(pnl_array), 2),
                "total_pnl": round(np.sum(pnl_array), 2),
                "wins": len(wins),
                "losses": len(valid_pnl) - len(wins),
                "best_trade": round(np.max(pnl_array), 2),
                "worst_trade": round(np.min(pnl_array), 2)
            }
            
        except Exception as e:
            logging.error(f"Failed to calculate trade stats: {e}")
            return {"count": 0, "win_rate": 0, "avg_pnl": 0}

    @staticmethod
    def get_csv_info(file_path: str) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ CSV —Ñ–∞–π–ª–µ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        try:
            if not os.path.exists(file_path):
                return {"exists": False}
            
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è
            file_stats = os.stat(file_path)
            
            # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ–±–æ–ª—å—à–æ–π
            if file_stats.st_size < 1024 * 1024:  # < 1MB
                data = CSVHandler.read_csv_cached(file_path, use_cache=True)
                columns = list(data[0].keys()) if data else []
                rows = len(data)
            else:
                # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ - —Ç–æ–ª—å–∫–æ –ø–æ–¥—Å—á–µ—Ç —Å—Ç—Ä–æ–∫
                with open(file_path, 'r', encoding='utf-8') as f:
                    rows = sum(1 for _ in f) - 1  # -1 –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    f.seek(0)
                    first_line = f.readline()
                    columns = first_line.strip().split(',') if first_line else []
            
            return {
                "exists": True,
                "rows": rows,
                "columns": columns,
                "file_size_kb": round(file_stats.st_size / 1024, 2),
                "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat()
            }
            
        except Exception as e:
            return {"exists": True, "error": str(e)}

    @staticmethod
    def force_flush():
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–æ—Å–∏—Ç—å –≤—Å–µ –±—É—Ñ–µ—Ä—ã"""
        _csv_batcher.force_flush()
        logging.info("üìÑ CSV buffers flushed manually")

    @staticmethod
    def get_batch_stats() -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á–∏–Ω–≥–∞"""
        return _csv_batcher.get_stats()

    @staticmethod
    def clear_cache():
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à —á—Ç–µ–Ω–∏—è"""
        CSVHandler._read_cache.clear()
        logging.info("üìÑ CSV read cache cleared")

    @staticmethod 
    def optimize_csv_file(file_path: str) -> bool:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è CSV —Ñ–∞–π–ª–∞ (—É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞)"""
        try:
            if not os.path.exists(file_path):
                return False
                
            # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            data = CSVHandler.read_csv_cached(file_path, use_cache=False)
            if not data:
                return False
            
            # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            df = pd.DataFrame(data)
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            initial_size = len(df)
            df = df.drop_duplicates()
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ timestamp –µ—Å–ª–∏ –µ—Å—Ç—å
            timestamp_cols = [col for col in df.columns if 'timestamp' in col.lower()]
            if timestamp_cols:
                try:
                    df[timestamp_cols[0]] = pd.to_datetime(df[timestamp_cols[0]], errors='coerce')
                    df = df.sort_values(timestamp_cols[0])
                except Exception:
                    pass
            
            # –°–æ–∑–¥–∞–µ–º –±—ç–∫–∞–ø
            backup_path = f"{file_path}.backup"
            os.rename(file_path, backup_path)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            df.to_csv(file_path, index=False)
            
            # –£–¥–∞–ª—è–µ–º –±—ç–∫–∞–ø –µ—Å–ª–∏ –≤—Å–µ –û–ö
            os.remove(backup_path)
            
            removed = initial_size - len(df)
            if removed > 0:
                logging.info(f"üìÑ Optimized {file_path}: removed {removed} duplicates")
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
            if file_path in CSVHandler._read_cache:
                del CSVHandler._read_cache[file_path]
            
            return True
            
        except Exception as e:
            logging.error(f"Failed to optimize CSV {file_path}: {e}")
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ –±—ç–∫–∞–ø–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
            backup_path = f"{file_path}.backup"
            if os.path.exists(backup_path):
                os.rename(backup_path, file_path)
            return False

# =============================================================================
# –£–¢–ò–õ–ò–¢–´ –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê
# =============================================================================

def get_csv_system_stats() -> Dict[str, Any]:
    """–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ CSV —Å–∏—Å—Ç–µ–º—ã"""
    return {
        "batch_writer": _csv_batcher.get_stats(),
        "read_cache": {
            "size": len(CSVHandler._read_cache),
            "files": list(CSVHandler._read_cache.keys())
        },
        "files": {
            "signals": CSVHandler.get_csv_info(SIGNALS_CSV),
            "trades": CSVHandler.get_csv_info(CLOSED_TRADES_CSV)
        }
    }

def maintenance_csv_system():
    """–û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ CSV —Å–∏—Å—Ç–µ–º—ã"""
    try:
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π flush
        CSVHandler.force_flush()
        
        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
        CSVHandler.clear_cache()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ (–µ—Å–ª–∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ)
        for file_path in [SIGNALS_CSV, CLOSED_TRADES_CSV]:
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                if file_size > 1024 * 1024:  # > 1MB
                    CSVHandler.optimize_csv_file(file_path)
        
        logging.info("üìÑ CSV system maintenance completed")
        return True
        
    except Exception as e:
        logging.error(f"CSV maintenance failed: {e}")
        return False

# =============================================================================
# –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨ –ò –ú–ò–ì–†–ê–¶–ò–Ø
# =============================================================================

# –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def log_signal(data):
    """–ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    return CSVHandler.log_signal_snapshot(data)

def log_closed_trade(data):
    """–ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    return CSVHandler.log_close_trade(data)

def read_csv(file_path):
    """–ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    return CSVHandler.read_csv_safe(file_path)

# –ê–≤—Ç–æ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
try:
    os.makedirs(LOGS_DIR, exist_ok=True)
    logging.info("üìÑ Optimized CSV Handler initialized with batching")
except Exception as e:
    logging.error(f"CSV Handler initialization failed: {e}")

# –≠–∫—Å–ø–æ—Ä—Ç
__all__ = [
    'CSVHandler',
    'get_csv_system_stats', 
    'maintenance_csv_system',
    '_csv_batcher'  # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
]