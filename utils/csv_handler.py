# utils/csv_handler.py - UNIFIED CACHE VERSION (–≠–¢–ê–ü 3)

import csv
import os
import logging
import pandas as pd
import threading
import time
from datetime import datetime
from typing import List, Dict, Any, NamedTuple
from collections import deque
from config.settings import CLOSED_TRADES_CSV, SIGNALS_CSV, LOGS_DIR

# ‚úÖ –≠–¢–ê–ü 3: UNIFIED CACHE INTEGRATION
try:
    from utils.unified_cache import get_cache_manager, CacheNamespace
    UNIFIED_CACHE_AVAILABLE = True
    logging.info("üìÑ CSV Handler: Unified Cache Manager loaded")
except ImportError:
    UNIFIED_CACHE_AVAILABLE = False
    logging.warning("üìÑ CSV Handler: Unified Cache not available, using fallback")

# =============================================================================
# –°–ò–°–¢–ï–ú–ê –ë–ê–¢–ß–ò–ù–ì–ê CSV –ó–ê–ü–ò–°–ï–ô (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# =============================================================================

class CSVRecord(NamedTuple):
    """–ó–∞–ø–∏—Å—å –¥–ª—è –±–∞—Ç—á–∏–Ω–≥–∞"""
    file_path: str
    fieldnames: List[str] 
    data: Dict[str, Any]

class BatchCSVWriter:
    """–ë–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –≤ CSV —Ñ–∞–π–ª—ã"""
    
    def __init__(self, batch_size: int = 10, flush_interval: float = 30.0):
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self._buffer = deque()
        self._lock = threading.RLock()
        self._last_flush = time.time()
        self._total_records = 0
        self._flush_count = 0
        self._thread = None
        self._stop = threading.Event()
        self._started = False
    
    def add_record(self, file_path: str, fieldnames: List[str], data: Dict[str, Any]):
        """–î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å—å –≤ –±—É—Ñ–µ—Ä"""
        with self._lock:
            self._buffer.append(CSVRecord(file_path, fieldnames, data))
            self._total_records += 1
            
            # –ù–µ–º–µ–¥–ª–µ–Ω–Ω—ã–π flush –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞
            if len(self._buffer) >= self.batch_size:
                self._flush_buffer()
    
    def _background_flush_loop(self):
        """–§–æ–Ω–æ–≤–∞—è –ø–µ—Ç–ª—è —Å–±—Ä–æ—Å–∞ –±—É—Ñ–µ—Ä–∞ (—É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è start/stop)"""
        while not self._stop.is_set():
            time.sleep(self.flush_interval)
            with self._lock:
                if self._buffer and time.time() - self._last_flush > self.flush_interval:
                    self._flush_buffer()
        # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π flush –Ω–∞ –≤—ã—Ö–æ–¥–µ
        with self._lock:
            if self._buffer:
                self._flush_buffer()
        logging.debug("üìÑ Background CSV flusher stopped")
    
    def start(self) -> bool:
        """–Ø–≤–Ω—ã–π –∑–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ —Ñ–ª–∞—à–µ—Ä–∞ (–∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ)."""
        if getattr(self, "_started", False):
            return False
        self._stop.clear()
        self._thread = threading.Thread(target=self._background_flush_loop, daemon=True, name="CSVBatchFlusher")
        self._thread.start()
        self._started = True
        logging.debug("üìÑ Background CSV flusher started")
        return True
    
    def stop(self, timeout: float = 2.0) -> bool:
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ñ–æ–Ω–æ–≤–æ–≥–æ —Ñ–ª–∞—à–µ—Ä–∞ —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º."""
        if not getattr(self, "_started", False):
            return False
        self._stop.set()
        if self._thread:
            self._thread.join(timeout=timeout)
        self._started = False
        return True
    
    def _flush_buffer(self):
        """–°–±—Ä–æ—Å–∏—Ç—å –±—É—Ñ–µ—Ä –≤ —Ñ–∞–π–ª—ã"""
        if not self._buffer:
            return
            
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ –ø–æ —Ñ–∞–π–ª–∞–º –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        by_file = {}
        buffer_size = len(self._buffer)
        
        while self._buffer:
            record = self._buffer.popleft()
            if record.file_path not in by_file:
                by_file[record.file_path] = {
                    'fieldnames': record.fieldnames,
                    'records': []
                }
            by_file[record.file_path]['records'].append(record.data)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –ø–æ —Ñ–∞–π–ª–∞–º
        for file_path, file_data in by_file.items():
            try:
                self._write_batch_to_file(file_path, file_data['fieldnames'], file_data['records'])
            except Exception as e:
                logging.error(f"Batch CSV write failed for {file_path}: {e}")
        
        self._last_flush = time.time()
        self._flush_count += 1
        
        logging.debug(f"üìÑ CSV batch flushed: {buffer_size} records to {len(by_file)} files")
    
    def _write_batch_to_file(self, file_path: str, fieldnames: List[str], records: List[Dict[str, Any]]):
        """–ó–∞–ø–∏—Å–∞—Ç—å –±–∞—Ç—á –∑–∞–ø–∏—Å–µ–π –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª"""
        if not records:
            return
            
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ —Ñ–∞–π–ª –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        os.makedirs(os.path.dirname(file_path) or ".", exist_ok=True)
        
        file_exists = os.path.isfile(file_path)
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–æ–≤—ã–π
        if not file_exists:
            with open(file_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ –æ–¥–Ω–∏–º –±–ª–æ–∫–æ–º
        with open(file_path, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            for record_data in records:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –ø–æ–ª—è–º –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç—ã–µ
                filtered_data = {}
                for field in fieldnames:
                    value = record_data.get(field, "")
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è CSV
                    if isinstance(value, (int, float)) and not isinstance(value, bool):
                        filtered_data[field] = str(value) if pd.notna(value) else ""
                    else:
                        filtered_data[field] = str(value) if value is not None else ""
                        
                writer.writerow(filtered_data)
    
    def force_flush(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–æ—Å–∏—Ç—å –±—É—Ñ–µ—Ä"""
        with self._lock:
            self._flush_buffer()
    
    def get_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á–µ—Ä–∞"""
        with self._lock:
            return {
                "buffer_size": len(self._buffer),
                "total_records": self._total_records,
                "flush_count": self._flush_count,
                "last_flush": self._last_flush,
                "batch_size": self.batch_size,
                "flush_interval": self.flush_interval
            }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –±–∞—Ç—á–µ—Ä
_csv_batcher = BatchCSVWriter(batch_size=15, flush_interval=20.0)

# =============================================================================
# ‚úÖ –≠–¢–ê–ü 3: CSV HANDLER –° UNIFIED CACHE
# =============================================================================

class CSVHandler:
    """CSV –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å UNIFIED CACHE —Å–∏—Å—Ç–µ–º–æ–π"""
    
    # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Å–∏–≥–Ω–∞–ª–æ–≤
    SIGNALS_FIELDS = [
        "timestamp", "symbol", "timeframe", "close", 
        "buy_score", "ai_score", "market_condition", "decision", "reason"
    ]

    # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Å–¥–µ–ª–æ–∫  
    TRADES_FIELDS = [
        "timestamp_open", "timestamp_close", "symbol", "side",
        "entry_price", "exit_price", "qty_usd", "pnl_pct", "pnl_abs", 
        "duration_minutes", "reason", "buy_score", "ai_score"
    ]

    # ‚úÖ –ù–û–í–û–ï: Unified Cache –≤–º–µ—Å—Ç–æ _read_cache
    @staticmethod
    def _get_cache_manager():
        """–ü–æ–ª—É—á–∏—Ç—å unified cache manager —Å fallback"""
        if UNIFIED_CACHE_AVAILABLE:
            return get_cache_manager()
        return None

    @staticmethod 
    def _create_cache_key(file_path: str, use_mtime: bool = True) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –∫—ç—à–∞ —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞"""
        try:
            if use_mtime and os.path.exists(file_path):
                mtime = os.path.getmtime(file_path)
                file_size = os.path.getsize(file_path)
                return f"{file_path}:{mtime}:{file_size}"
            else:
                return file_path
        except Exception:
            return file_path
    
    @staticmethod
    def log_signal_snapshot(data: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞ —Å –±–∞—Ç—á–∏–Ω–≥–æ–º"""
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º timestamp –µ—Å–ª–∏ –Ω–µ—Ç
            if "timestamp" not in data:
                data["timestamp"] = datetime.now().isoformat()
                
            _csv_batcher.add_record(SIGNALS_CSV, CSVHandler.SIGNALS_FIELDS, data)
            logging.debug("üìä Signal logged to batch")
            
        except Exception as e:
            logging.error(f"Failed to log signal: {e}")
    
    @staticmethod  
    def log_close_trade(data: Dict[str, Any]):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–∫—Ä—ã—Ç–æ–π —Å–¥–µ–ª–∫–∏ —Å –±–∞—Ç—á–∏–Ω–≥–æ–º"""
        try:
            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ —É–ø—Ä–æ—â–µ–Ω–Ω—É—é —Å—Ö–µ–º—É
            adapted = {
                "timestamp_open": data.get("entry_ts", ""),
                "timestamp_close": data.get("close_ts", datetime.now().isoformat()),
                "symbol": data.get("symbol", ""),
                "side": data.get("side", "LONG"),
                "entry_price": data.get("entry_price", 0.0),
                "exit_price": data.get("exit_price", 0.0),
                "qty_usd": data.get("qty_usd", 0.0),
                "pnl_pct": data.get("pnl_pct", 0.0),
                "pnl_abs": data.get("pnl_abs", 0.0),
                "duration_minutes": data.get("duration_minutes", 0.0),
                "reason": data.get("reason", ""),
                "buy_score": data.get("buy_score"),
                "ai_score": data.get("ai_score")
            }
            
            _csv_batcher.add_record(CLOSED_TRADES_CSV, CSVHandler.TRADES_FIELDS, adapted)
            logging.debug("üí∞ Trade logged to batch")
            
        except Exception as e:
            logging.error(f"Failed to log trade: {e}")

    @staticmethod
    def read_csv_cached(file_path: str, use_cache: bool = True) -> List[Dict[str, Any]]:
        """‚úÖ –≠–¢–ê–ü 3: –ß—Ç–µ–Ω–∏–µ CSV —Å UNIFIED CACHE"""
        if not os.path.exists(file_path):
            return []
        
        # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º unified cache –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        cache_manager = CSVHandler._get_cache_manager()
        
        if use_cache and cache_manager and UNIFIED_CACHE_AVAILABLE:
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
            cache_key = CSVHandler._create_cache_key(file_path, use_mtime=True)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º unified cache
            cached_data = cache_manager.get(cache_key, CacheNamespace.CSV_READS)
            if cached_data is not None:
                logging.debug(f"üìÑ CSV Cache HIT (unified): {file_path}")
                return cached_data.copy()
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Å –¥–∏—Å–∫–∞
        try:
            with open(file_path, mode="r", newline="", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                data = list(reader)
            
            # ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ unified cache
            if use_cache and cache_manager and UNIFIED_CACHE_AVAILABLE:
                cache_key = CSVHandler._create_cache_key(file_path, use_mtime=True)
                success = cache_manager.set(
                    cache_key, 
                    data.copy(), 
                    CacheNamespace.CSV_READS,
                    metadata={"file_path": file_path, "rows": len(data)}
                )
                if success:
                    logging.debug(f"üìÑ CSV Cache SET (unified): {file_path} ({len(data)} rows)")
                else:
                    logging.warning(f"üìÑ CSV Cache SET failed: {file_path}")
            
            return data
            
        except Exception as e:
            logging.error(f"Failed to read CSV {file_path}: {e}")
            return []

    @staticmethod
    def read_csv_safe(file_path: str) -> List[Dict[str, Any]]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Å unified –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        return CSVHandler.read_csv_cached(file_path, use_cache=True)

    @staticmethod
    def read_last_trades(limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–¥–µ–ª–∫–∏ —Å unified cache –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        try:
            trades = CSVHandler.read_csv_cached(CLOSED_TRADES_CSV, use_cache=True)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ —Å–¥–µ–ª–∫–∏
            completed = []
            for trade in trades:
                exit_price = trade.get("exit_price", "")
                if exit_price and str(exit_price).strip() and str(exit_price) != "0.0":
                    completed.append(trade)
            
            return completed[-limit:] if completed else []
            
        except Exception as e:
            logging.error(f"Failed to get last trades: {e}")
            return []

    @staticmethod
    def get_trade_stats() -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–¥–µ–ª–∫–∞–º —Å unified cache"""
        try:
            trades = CSVHandler.read_csv_cached(CLOSED_TRADES_CSV, use_cache=True)
            if not trades:
                return {"count": 0, "win_rate": 0, "avg_pnl": 0}

            # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Ä–∞—Å—á–µ—Ç
            valid_pnl = []
            for trade in trades:
                exit_price = trade.get("exit_price", "")
                if exit_price and str(exit_price).strip():
                    try:
                        pnl = float(trade.get("pnl_pct", 0))
                        valid_pnl.append(pnl)
                    except (ValueError, TypeError):
                        continue

            if not valid_pnl:
                return {"count": 0, "win_rate": 0, "avg_pnl": 0}

            # –ë—ã—Å—Ç—Ä—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
            import numpy as np
            pnl_array = np.array(valid_pnl)
            wins = pnl_array[pnl_array > 0]
            
            return {
                "count": len(valid_pnl),
                "win_rate": round((len(wins) / len(valid_pnl)) * 100, 2),
                "avg_pnl": round(np.mean(pnl_array), 2),
                "total_pnl": round(np.sum(pnl_array), 2),
                "wins": len(wins),
                "losses": len(valid_pnl) - len(wins),
                "best_trade": round(np.max(pnl_array), 2),
                "worst_trade": round(np.min(pnl_array), 2)
            }
            
        except Exception as e:
            logging.error(f"Failed to calculate trade stats: {e}")
            return {"count": 0, "win_rate": 0, "avg_pnl": 0}

    @staticmethod
    def get_csv_info(file_path: str) -> Dict[str, Any]:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ CSV —Ñ–∞–π–ª–µ —Å unified –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        try:
            if not os.path.exists(file_path):
                return {"exists": False}
            
            # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è
            file_stats = os.stat(file_path)
            
            # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ–±–æ–ª—å—à–æ–π
            if file_stats.st_size < 1024 * 1024:  # < 1MB
                data = CSVHandler.read_csv_cached(file_path, use_cache=True)
                columns = list(data[0].keys()) if data else []
                rows = len(data)
            else:
                # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ - —Ç–æ–ª—å–∫–æ –ø–æ–¥—Å—á–µ—Ç —Å—Ç—Ä–æ–∫
                with open(file_path, 'r', encoding='utf-8') as f:
                    rows = sum(1 for _ in f) - 1  # -1 –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    f.seek(0)
                    first_line = f.readline()
                    columns = first_line.strip().split(',') if first_line else []
            
            return {
                "exists": True,
                "rows": rows,
                "columns": columns,
                "file_size_kb": round(file_stats.st_size / 1024, 2),
                "modified": datetime.fromtimestamp(file_stats.st_mtime).isoformat()
            }
            
        except Exception as e:
            return {"exists": True, "error": str(e)}

    @staticmethod
    def force_flush():
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–æ—Å–∏—Ç—å –≤—Å–µ –±—É—Ñ–µ—Ä—ã"""
        _csv_batcher.force_flush()
        logging.info("üìÑ CSV buffers flushed manually")

    @staticmethod
    def get_batch_stats() -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞—Ç—á–∏–Ω–≥–∞"""
        return _csv_batcher.get_stats()

    @staticmethod
    def clear_cache():
        """‚úÖ –≠–¢–ê–ü 3: –û—á–∏—Å—Ç–∏—Ç—å unified cache CSV namespace"""
        cache_manager = CSVHandler._get_cache_manager()
        
        if cache_manager and UNIFIED_CACHE_AVAILABLE:
            # –û—á–∏—â–∞–µ–º –≤–µ—Å—å namespace CSV_READS
            cache_manager.clear_namespace(CacheNamespace.CSV_READS)
            logging.info("üìÑ CSV unified cache cleared (namespace CSV_READS)")
        else:
            logging.info("üìÑ CSV cache clear skipped: unified cache not available")

    @staticmethod 
    def optimize_csv_file(file_path: str) -> bool:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è CSV —Ñ–∞–π–ª–∞ (—É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞)"""
        try:
            if not os.path.exists(file_path):
                return False
                
            # –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–Ω–µ –∏–∑ –∫—ç—à–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
            data = CSVHandler.read_csv_cached(file_path, use_cache=False)
            if not data:
                return False
            
            # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            df = pd.DataFrame(data)
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            initial_size = len(df)
            df = df.drop_duplicates()
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ timestamp –µ—Å–ª–∏ –µ—Å—Ç—å
            timestamp_cols = [col for col in df.columns if 'timestamp' in col.lower()]
            if timestamp_cols:
                try:
                    df[timestamp_cols[0]] = pd.to_datetime(df[timestamp_cols[0]], errors='coerce')
                    df = df.sort_values(timestamp_cols[0])
                except Exception:
                    pass
            
            # –°–æ–∑–¥–∞–µ–º –±—ç–∫–∞–ø
            backup_path = f"{file_path}.backup"
            os.rename(file_path, backup_path)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            df.to_csv(file_path, index=False)
            
            # –£–¥–∞–ª—è–µ–º –±—ç–∫–∞–ø –µ—Å–ª–∏ –≤—Å–µ –û–ö
            os.remove(backup_path)
            
            removed = initial_size - len(df)
            if removed > 0:
                logging.info(f"üìÑ Optimized {file_path}: removed {removed} duplicates")
            
            # ‚úÖ –û—á–∏—â–∞–µ–º unified cache –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
            CSVHandler._invalidate_file_cache(file_path)
            
            return True
            
        except Exception as e:
            logging.error(f"Failed to optimize CSV {file_path}: {e}")
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ –±—ç–∫–∞–ø–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
            backup_path = f"{file_path}.backup"
            if os.path.exists(backup_path):
                os.rename(backup_path, file_path)
            return False

    @staticmethod
    def _invalidate_file_cache(file_path: str):
        """‚úÖ –ù–û–í–û–ï: –ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫—ç—à–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        cache_manager = CSVHandler._get_cache_manager()
        
        if cache_manager and UNIFIED_CACHE_AVAILABLE:
            # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è —Ñ–∞–π–ª–∞
            possible_keys = [
                CSVHandler._create_cache_key(file_path, use_mtime=True),
                CSVHandler._create_cache_key(file_path, use_mtime=False),
                file_path
            ]
            
            for key in possible_keys:
                cache_manager.delete(key, CacheNamespace.CSV_READS)
            
            logging.debug(f"üìÑ Invalidated unified cache for: {file_path}")

    # =========================================================================
    # ‚úÖ –ù–û–í–´–ï –ú–ï–¢–û–î–´: UNIFIED CACHE –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
    # =========================================================================

    @staticmethod
    def get_cache_diagnostics() -> Dict[str, Any]:
        """‚úÖ –ù–û–í–û–ï: –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ unified cache –¥–ª—è CSV"""
        cache_manager = CSVHandler._get_cache_manager()
        
        if not cache_manager or not UNIFIED_CACHE_AVAILABLE:
            return {
                "unified_cache_available": False,
                "fallback_mode": True
            }
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats = cache_manager.get_stats()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø –∫–ª—é—á–µ–π –¥–ª—è CSV namespace
            top_keys = cache_manager.get_top_keys(CacheNamespace.CSV_READS, limit=5)
            
            return {
                "unified_cache_available": True,
                "csv_namespace_stats": stats["namespaces"].get("csv_reads", {}),
                "global_stats": stats["global"],
                "top_csv_keys": top_keys,
                "memory_pressure": stats["memory_pressure"]
            }
            
        except Exception as e:
            return {
                "unified_cache_available": True,
                "error": str(e)
            }

    @staticmethod
    def test_unified_cache_integration():
        """‚úÖ –ù–û–í–û–ï: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ unified cache –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        cache_manager = CSVHandler._get_cache_manager()
        
        if not cache_manager or not UNIFIED_CACHE_AVAILABLE:
            return {
                "test_passed": False,
                "reason": "Unified cache not available"
            }
        
        try:
            # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            test_key = "test_csv_file.csv"
            test_data = [{"col1": "value1", "col2": "value2"}]
            
            # –¢–µ—Å—Ç SET
            set_success = cache_manager.set(test_key, test_data, CacheNamespace.CSV_READS)
            
            # –¢–µ—Å—Ç GET
            retrieved_data = cache_manager.get(test_key, CacheNamespace.CSV_READS)
            
            # –¢–µ—Å—Ç DELETE
            delete_success = cache_manager.delete(test_key, CacheNamespace.CSV_READS)
            
            test_passed = (
                set_success and 
                retrieved_data == test_data and 
                delete_success
            )
            
            return {
                "test_passed": test_passed,
                "set_success": set_success,
                "get_success": retrieved_data == test_data,
                "delete_success": delete_success,
                "cache_stats": cache_manager.get_stats()["namespaces"].get("csv_reads", {})
            }
            
        except Exception as e:
            return {
                "test_passed": False,
                "error": str(e)
            }

# =============================================================================
# –£–¢–ò–õ–ò–¢–´ –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ)
# =============================================================================

def get_csv_system_stats() -> Dict[str, Any]:
    """‚úÖ –û–ë–ù–û–í–õ–ï–ù–û: –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ CSV —Å–∏—Å—Ç–µ–º—ã —Å unified cache"""
    base_stats = {
        "batch_writer": _csv_batcher.get_stats(),
        "files": {
            "signals": CSVHandler.get_csv_info(SIGNALS_CSV),
            "trades": CSVHandler.get_csv_info(CLOSED_TRADES_CSV)
        }
    }
    
    # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º unified cache —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    cache_diagnostics = CSVHandler.get_cache_diagnostics()
    base_stats["unified_cache"] = cache_diagnostics
    
    return base_stats

def maintenance_csv_system():
    """‚úÖ –û–ë–ù–û–í–õ–ï–ù–û: –û–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ CSV —Å–∏—Å—Ç–µ–º—ã —Å unified cache"""
    try:
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π flush
        CSVHandler.force_flush()
        
        # ‚úÖ –û—á–∏—Å—Ç–∫–∞ unified cache
        CSVHandler.clear_cache()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ (–µ—Å–ª–∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ)
        for file_path in [SIGNALS_CSV, CLOSED_TRADES_CSV]:
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                if file_size > 1024 * 1024:  # > 1MB
                    CSVHandler.optimize_csv_file(file_path)
        
        logging.info("üìÑ CSV system maintenance completed (with unified cache)")
        return True
        
    except Exception as e:
        logging.error(f"CSV maintenance failed: {e}")
        return False

# =============================================================================
# –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨ –ò –ú–ò–ì–†–ê–¶–ò–Ø (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# =============================================================================

# –ê–ª–∏–∞—Å—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def log_signal(data):
    """–ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    return CSVHandler.log_signal_snapshot(data)

def log_closed_trade(data):
    """–ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    return CSVHandler.log_close_trade(data)

def read_csv(file_path):
    """–ê–ª–∏–∞—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    return CSVHandler.read_csv_safe(file_path)

# –ê–≤—Ç–æ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
try:
    os.makedirs(LOGS_DIR, exist_ok=True)
    
    # ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º unified cache –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    if UNIFIED_CACHE_AVAILABLE:
        test_result = CSVHandler.test_unified_cache_integration()
        if test_result["test_passed"]:
            logging.info("üìÑ CSV Handler initialized with UNIFIED CACHE (‚úÖ test passed)")
        else:
            logging.warning(f"üìÑ CSV Handler: unified cache test failed - {test_result}")
    else:
        logging.info("üìÑ CSV Handler initialized in FALLBACK mode")
        
except Exception as e:
    logging.error(f"CSV Handler initialization failed: {e}")

# –≠–∫—Å–ø–æ—Ä—Ç
__all__ = [
    'CSVHandler',
    'get_csv_system_stats', 
    'maintenance_csv_system',
    '_csv_batcher'  # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
]